{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ffa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-genai faiss-cpu PyPDF2 python-dotenv numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f06ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión con Gemini establecida.\n"
     ]
    }
   ],
   "source": [
    "# Configuración y conexión con el LLM\n",
    "# Propósito: Inicializar la API de Gemini, cargar la API key y seleccionar el modelo\n",
    "\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import faiss\n",
    "import PyPDF2\n",
    "import google.genai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"No se encontró la API key en variables de entorno\")\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "print(\"Conexión con Gemini establecida.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884bfc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto cargado con 38 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Carga del PDF y división en chunks\n",
    "# Propósito: Leer la guía turística y dividirla en fragmentos para RAG\n",
    "\n",
    "PDF_PATH = \"TENERIFE.pdf\"\n",
    "\n",
    "def load_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "text = load_pdf(PDF_PATH)\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "print(f\"Texto cargado con {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c17453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando embeddings...\n",
      "FAISS index creado con 38 vectores.\n"
     ]
    }
   ],
   "source": [
    "# Embeddings y FAISS\n",
    "# Propósito: Convertir los chunks en vectores y construir un índice FAISS para búsqueda rápida\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\",\n",
    "        contents=text\n",
    "    )\n",
    "    return np.array(response.embeddings[0].values, dtype=\"float32\")\n",
    "\n",
    "print(\"Generando embeddings...\")\n",
    "\n",
    "embeddings = [get_embedding(chunk) for chunk in chunks]\n",
    "\n",
    "dimension = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "for emb in embeddings:\n",
    "    index.add(emb.reshape(1, -1))\n",
    "\n",
    "print(f\"FAISS index creado con {index.ntotal} vectores.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Búsqueda semántica\n",
    "# Propósito: Dada una pregunta, recuperar los chunks más relevantes del índice FAISS\n",
    "\n",
    "\n",
    "def search_chunks(query, top_k=3):\n",
    "    query_emb = get_embedding(query).reshape(1, -1)\n",
    "    distances, indices = index.search(query_emb, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for i in indices[0]:\n",
    "        results.append((i, chunks[i]))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ee547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función externa simulada\n",
    "# Propósito: Implementar la función get_weather para devolver predicciones de clima\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "weather_function_schema = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Obtiene la predicción del tiempo para una fecha concreta.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"fecha\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Fecha en formato YYYY-MM-DD\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"fecha\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_weather(fecha: str) -> str:\n",
    "    logging.info(f\"Llamada a get_weather con fecha: {fecha}\")\n",
    "    \n",
    "    weather_data = {\n",
    "        \"2024-05-01\": \"Soleado, 26°C\",\n",
    "        \"2024-05-02\": \"Parcialmente nublado, 24°C\",\n",
    "        \"2024-05-03\": \"Lluvia ligera, 22°C\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        return weather_data.get(fecha, \"No hay datos disponibles para esa fecha.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en get_weather: {e}\")\n",
    "        return \"Error consultando el servicio meteorológico.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f59b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memoria de conversación\n",
    "# Propósito: Guardar el historial y construir contexto para el LLM\n",
    "\n",
    "chat_history = []\n",
    "MAX_TURNS = 5\n",
    "\n",
    "def build_conversation_context():\n",
    "    history_text = \"\"\n",
    "    for turn in chat_history[-MAX_TURNS:]:\n",
    "        history_text += f\"Usuario: {turn['user']}\\n\"\n",
    "        history_text += f\"Asistente: {turn['assistant']}\\n\"\n",
    "    return history_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función principal de diálogo\n",
    "# Propósito: Combinar RAG, multiturno y function call para generar respuestas\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def chat_rag(user_input, temperature=0.7, max_tokens=500):\n",
    "    \n",
    "    # 1️⃣ Detectar intención clima\n",
    "    if \"clima\" in user_input.lower() or \"tiempo\" in user_input.lower():\n",
    "        match = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", user_input)\n",
    "        if match:\n",
    "            fecha = match.group()\n",
    "            weather_response = get_weather(fecha)\n",
    "            chat_history.append({\"user\": user_input, \"assistant\": weather_response})\n",
    "            return f\"Predicción del tiempo para {fecha}: {weather_response}\"\n",
    "    \n",
    "    # Recuperación RAG\n",
    "    retrieved_chunks = search_chunks(user_input)\n",
    "    \n",
    "    context_text = \"\"\n",
    "    for idx, chunk in retrieved_chunks:\n",
    "        context_text += f\"\\n[Chunk {idx}]\\n{chunk}\\n\"\n",
    "    \n",
    "    # Historial multiturno\n",
    "    conversation_context = build_conversation_context()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Eres un asistente turístico experto en Tenerife.\n",
    "\n",
    "Historial:\n",
    "{conversation_context}\n",
    "\n",
    "Contexto relevante:\n",
    "{context_text}\n",
    "\n",
    "Pregunta:\n",
    "{user_input}\n",
    "\n",
    "Responde citando la fuente como:\n",
    "Fuente: Chunk X\n",
    "\"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=prompt,\n",
    "        config={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_tokens,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    text = (response.text or \"\").strip()\n",
    "\n",
    "    chat_history.append({\"user\": user_input, \"assistant\": text})\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd567c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Teide es una visita obligada si estás en Tenerife. La recomendación personal es subir por la carretera TF24 (carretera de La Esperanza) desde la rotonda de Padre Anchieta en La Laguna.\n",
      "\n",
      "En el camino, puedes parar en el Mirador de La Tarta, llamado así por la forma en que la piedra de la curva parece una tarta con diferentes colores. Desde allí, si el norte está nublado, podrás disfrutar de un espectacular mar de nubes.\n",
      "\n",
      "Para subir al Teide, lo ideal es llegar hasta el Parador de las Cañadas del Teide y aparcar allí para dirigirte al mirador de La Ruleta, desde donde se obtienen las vistas más famosas de la isla. Si deseas ascender hasta el pico del Teide, puedes hacerlo utilizando los teleféricos desde este punto.\n",
      "\n",
      "Para obtener más información sobre el Teide, puedes visitar el Centro de Visitantes de El Portillo, que es gratuito. También existe la opción de subir de noche.\n",
      "\n",
      "Fuente: Chunk 19\n",
      "Fuente: Chunk 20\n",
      "Fuente: Chunk 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents \"HTTP/1.1 200 OK\"\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Llamada a get_weather con fecha: 2024-05-01\n",
      "INFO:root:Llamada a get_weather con fecha: 2024-12-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En la zona de ocio, te recomendaría la Playa de Torviscas o la Playa de Troya. Encima de la Playa de Troya se encuentra el Monkey Beach Club, uno de los beach clubs más populares de Tenerife.\n",
      "Fuente: Chunk 23\n",
      "\n",
      "De la zona de Los Cristianos, te recomendaría la Playa de Los Cristianos o la Playa de Las Vistas.\n",
      "Fuente: Chunk 30\n",
      "Predicción del tiempo para 2024-05-01: Soleado, 26°C\n",
      "Predicción del tiempo para 2024-12-01: No hay datos disponibles para esa fecha.\n"
     ]
    }
   ],
   "source": [
    "# Pruebas del asistente\n",
    "# Propósito: Probar que RAG, multiturno y función clima funcionan correctamente\n",
    "\n",
    "print(chat_rag(\"Háblame del Teide\"))\n",
    "print(chat_rag(\"¿Qué playas cercanas hay?\"))\n",
    "print(chat_rag(\"¿Qué tiempo hará el 2024-05-01?\"))\n",
    "print(chat_rag(\"¿Qué tiempo hará el 2024-12-01?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "267cef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de chunks: 38\n",
      "Número de vectores en FAISS: 38\n",
      "Historial actual: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de chunks:\", len(chunks))\n",
    "print(\"Número de vectores en FAISS:\", index.ntotal)\n",
    "print(\"Historial actual:\", len(chat_history))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
